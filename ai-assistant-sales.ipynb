{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Intelligence: Gong.io Open-Source Alternative AI Sales Assistant\n",
    "\n",
    "In this lesson we will explore how LangChain, Deep Lake, and GPT-4 can be used to develop a sales assistant able to give advice to salesman, taking into considerations internal guidelines.\n",
    "\n",
    "Introduction\n",
    "This particular article provides an in-depth view of a sales call assistant that connects you to a chatbot that understands the context of your conversation. A great feature of SalesCopilot is its ability to detect potential objections from customers and deliver recommendations on how best to address them.\n",
    "\n",
    "The article is a journey that reveals the challenges faced and the solutions discovered during the creation of a the project. You'll learn about the two distinct text-splitting approaches that didn't work and how these failures paved the way for an effective solution.\n",
    "\n",
    "Firstly, the authors tried to rely solely on the LLM, but they encountered issues such as response inconsistency and slow response times with GPT-4. Secondly, they naively split the custom knowledge base into chunks, but this approach led to context misalignment and inefficient results.\n",
    "\n",
    "After these unsuccessful attempts, a more intelligent way of splitting the knowledge base based on its structure was adopted. This change greatly improved the quality of responses and ensured better context grounding for LLM responses. This process is explained in detail, helping you grasp how to navigate similar challenges in your own AI projects.\n",
    "\n",
    "Next, the article explores how SalesCopilot was integrated with Deep Lake. This integration enhanced SalesCopilot's capabilities by retrieving the most relevant responses from a custom knowledge base, thereby creating a persistent, efficient, and highly adaptable solution for handling customer objections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did Work: Intelligent Splitting\n",
    "\n",
    "In our example text, there is a set structure to each individual objection and its recommended response. Rather than split the text based on size, why don’t we split the text based on its structure? We want each chunk to begin with the objection, and end before the “Objection” of the next chunk. Here’s how we could do it:\n",
    "\n",
    "*First, we take our knowledge base and embed it, storing the embeddings in a Deep Lake vector database. Then, when we detect an objection in the transcript, we embed the objection and use it to search our database, retrieving the most similar guidelines. We then pass those guidelines along with the objection to the LLM and send the result to the user.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating, Loading, and Querying Our Database for AI\n",
    "\n",
    "We’re going to define a class that handles the database creation, database loading, and database querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "class DeepLakeLoader:\n",
    "    def __init__(self, source_data_path):\n",
    "        self.source_data_path = source_data_path\n",
    "        self.file_name = os.path.basename(source_data_path) # What we'll name our database \n",
    "        self.data = self.split_data()\n",
    "        if self.check_if_db_exists():\n",
    "            self.db = self.load_db()\n",
    "        else:\n",
    "            self.db = self.create_db()\n",
    "\n",
    "    def split_data(self):  \n",
    "        \"\"\"  \n",
    "        Preprocess the data by splitting it into passages.  \n",
    "\n",
    "        If using a different data source, this function will need to be modified.  \n",
    "\n",
    "        Returns:  \n",
    "            split_data (list): List of passages.  \n",
    "        \"\"\"  \n",
    "        with open(self.source_data_path, 'r') as f:  \n",
    "            content = f.read()  \n",
    "        split_data = re.split(r'(?=\\d+\\. )', content)\n",
    "        if split_data[0] == '':  \n",
    "            split_data.pop(0)  \n",
    "        split_data = [entry for entry in split_data if len(entry) >= 30]  \n",
    "        return split_data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s a few things happening here. First, the data is being processed by a method called split_data.\n",
    "\n",
    "Since we know the structure of our knowledge base, we use this method to split it into individual entries, each representing an example of a customer objection. When we run our similarity search using the detected customer objection, this will improve the results, as outlined above.\n",
    "\n",
    "After preprocessing the data, we check if we’ve already created a database for this data. One of the great things about Deep Lake is that it provides us with persistent storage, so we only need to create the database once. If you restart the app, the database doesn’t disappear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and loading the database is super easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(self):  \n",
    "    \"\"\"  \n",
    "    Load the database if it already exists.  \n",
    "\n",
    "    Returns:  \n",
    "        DeepLake: DeepLake object.  \n",
    "    \"\"\"  \n",
    "    return DeepLake(dataset_path=f'deeplake/{self.file_name}', embedding_function=OpenAIEmbeddings(), read_only=True)  \n",
    "\n",
    "def create_db(self):  \n",
    "    \"\"\"  \n",
    "    Create the database if it does not already exist.  \n",
    "\n",
    "    Databases are stored in the deeplake directory.  \n",
    "\n",
    "    Returns:  \n",
    "        DeepLake: DeepLake object.  \n",
    "    \"\"\"  \n",
    "    return DeepLake.from_texts(self.data, OpenAIEmbeddings(), dataset_path=f'deeplake/{self.file_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like that, our knowledge base becomes a vector database that we can now query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(self, query):  \n",
    "    \"\"\"  \n",
    "    Query the database for passages that are similar to the query.  \n",
    "\n",
    "    Args:  \n",
    "        query (str): Query string.  \n",
    "\n",
    "    Returns:  \n",
    "        content (list): List of passages that are similar to the query.  \n",
    "    \"\"\"  \n",
    "    results = self.db.similarity_search(query, k=3)  \n",
    "    content = []  \n",
    "    for result in results:  \n",
    "        content.append(result.page_content)  \n",
    "    return content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Our Database to GPT-4\n",
    "\n",
    "Now, all we need to do is connect our LLM to the database. First, we need to create a DeepLakeLoader instance with the path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from Deeplake\n",
    "db = DeepLakeLoader('data/salestesting.txt')\n",
    "# Run the LLM\n",
    "results = db.query_db(detected_objection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have our LLM generate a message based off these results and the objection, we use LangChain. In this example, we use a placeholder for the prompt - if you want to check out the prompts used in SalesCopilot, check out the prompts.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "system_message = SystemMessage(content=objection_prompt)\n",
    "human_message = HumanMessage(content=f'Customer objection: {detected_objection} | Relevant guidelines: {results}')\n",
    "\n",
    "response = chat([system_message, human_message])\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
