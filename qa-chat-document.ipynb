{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Question Answering Chatbot over Documents with Sources\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let’s explore a more advanced application of Artificial Intelligence - building a Question Answering (QA) Chatbot that works over documents and provides sources of information for its answers. Our QA Chatbot uses a chain (specifically, the RetrievalQAWithSourcesChain), and leverages it to sift through a collection of documents, extracting relevant information to answer queries.\n",
    "\n",
    "The chain sends structured prompts to the underlying language model to generate responses. These prompts are crafted to guide the language model's generation, thereby improving the quality and relevance of the responses. Additionally, the retrieval chain is designed to keep track of the sources of information it retrieves to provide answers, offering the ability to back up its responses with credible references.\n",
    "\n",
    "As we proceed, we'll learn how to:\n",
    "\n",
    "Scrape online articles and store each article's text content and URL.\n",
    "Use an embedding model to compute embeddings of these documents and store them in Deep Lake, a vector database.\n",
    "Split the article texts into smaller chunks, keeping track of each chunk's source.\n",
    "Utilize RetrievalQAWithSourcesChain to create a chatbot that retrieves answers and tracks their sources.\n",
    "Generate a response to a query using the chain and display the answer along with its sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping for the News\n",
    "\n",
    "Now, let's begin by fetching some articles related to AI news. We're particularly interested in the text content of each article and the URL where it was published.\n",
    "\n",
    "In the code, you’ll see the following:\n",
    "\n",
    "- Imports: We begin by importing necessary Python libraries. requests are used to send HTTP requests, the newspaper is a fantastic tool for extracting and curating articles from a webpage, and time will help us introduce pauses during our web scraping task.\n",
    "- Headers: Some websites may block requests without a proper User-Agent header as they may consider it as a bot's action. Here we define a User-Agent string to mimic a real browser's request.\n",
    "- Article URLs: We have a list of URLs for online articles related to artificial intelligence news that we wish to scrape.\n",
    "- Web Scraping: We create an HTTP session using requests.Session() allows us to make multiple requests within the same session. We also define an empty list of pages_content to store our scraped articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article # https://github.com/codelucas/newspaper\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_urls = [\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/16/openai-ceo-ai-regulation-is-essential/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/15/jay-migliaccio-ibm-watson-on-leveraging-ai-to-improve-productivity/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/15/iurii-milovanov-softserve-how-ai-ml-is-helping-boost-innovation-and-personalisation/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/11/ai-and-big-data-expo-north-america-begins-in-less-than-one-week/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/05/02/ai-godfather-warns-dangers-and-quits-google/\",\n",
    "    \"https://www.artificialintelligence-news.com/2023/04/28/palantir-demos-how-ai-can-used-military/\"\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "pages_content = [] # where we save the scraped articles\n",
    "\n",
    "for url in article_urls:\n",
    "    try:\n",
    "        time.sleep(2) # sleep two seconds for gentle scraping\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            article = Article(url)\n",
    "            article.download() # download HTML of webpage\n",
    "            article.parse() # parse HTML to extract the article text\n",
    "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
    "        else:\n",
    "            print(f\"Failed to fetch article at {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while fetching article at {url}: {e}\")\n",
    "\n",
    "#If an error occurs while fetching an article, we catch the exception and print\n",
    "#an error message. This ensures that even if one article fails to download,\n",
    "#the rest of the articles can still be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll compute the embeddings of our documents using an embedding model and store them in Deep Lake, a multimodal vector database. OpenAIEmbeddings will be used to generate vector representations of our documents. These embeddings are high-dimensional vectors that capture the semantic content of the documents. When we create an instance of the Deep Lake class, we provide a path that starts with hub://... that specifies the database name, which will be stored on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n",
      "c:\\Users\\edumu\\anaconda3\\envs\\llm-lc\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/edumunozsala/langchain_course_qabot_with_source\n",
      "hub://edumunozsala/langchain_course_qabot_with_source loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "#Use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"edumunozsala\"\n",
    "my_activeloop_dataset_name = \"langchain_course_qabot_with_source\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is a crucial part of the setup because it prepares the system for storing and retrieving the documents based on their semantic content. This functionality is key for the following steps, where we’d find the most relevant documents to answer a user's question.\n",
    "\n",
    "Then, we'll break down these articles into smaller chunks, and for each chunk, we'll save its corresponding URL as a source. This division helps in efficiently processing the data, making the retrieval task more manageable, and focusing on the most relevant pieces of text when answering a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the article texts into small chunks. While doing so, we keep track of each\n",
    "# chunk metadata (i.e. the URL where it comes from). Each metadata is a dictionary and\n",
    "# we need to use the \"source\" key for the document source so that we can then use the\n",
    "# RetrievalQAWithSourcesChain class which will automatically retrieve the \"source\" item\n",
    "# from the metadata dictionary.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "for d in pages_content:\n",
    "    chunks = text_splitter.split_text(d[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({ \"source\": d[\"url\"] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"source\" key is used in the metadata dictionary to align with the RetrievalQAWithSourcesChain class's expectations, which will automatically retrieve this \"source\" item from the metadata. We then add these chunks to our Deep Lake database along with their respective metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 1/1 [00:14<00:00\n",
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://edumunozsala/langchain_course_qabot_with_source', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape      dtype  compression\n",
      "  -------   -------   -------    -------  ------- \n",
      " embedding  generic  (49, 1536)  float32   None   \n",
      "    ids      text     (49, 1)      str     None   \n",
      " metadata    json     (49, 1)      str     None   \n",
      "   text      text     (49, 1)      str     None   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['f9d88231-71b8-11ee-bf2f-cc2f714963ed',\n",
       " 'f9d88232-71b8-11ee-afd6-cc2f714963ed',\n",
       " 'f9d88233-71b8-11ee-9abc-cc2f714963ed',\n",
       " 'f9d8a94e-71b8-11ee-8176-cc2f714963ed',\n",
       " 'f9d8a94f-71b8-11ee-9882-cc2f714963ed',\n",
       " 'f9d8a950-71b8-11ee-8a14-cc2f714963ed',\n",
       " 'f9d8a951-71b8-11ee-a58a-cc2f714963ed',\n",
       " 'f9d8a952-71b8-11ee-8f4f-cc2f714963ed',\n",
       " 'f9d8a953-71b8-11ee-ba0b-cc2f714963ed',\n",
       " 'f9d8a954-71b8-11ee-86e7-cc2f714963ed',\n",
       " 'f9d8a955-71b8-11ee-b56d-cc2f714963ed',\n",
       " 'f9d8a956-71b8-11ee-b562-cc2f714963ed',\n",
       " 'f9d8a957-71b8-11ee-8352-cc2f714963ed',\n",
       " 'f9d8a958-71b8-11ee-8703-cc2f714963ed',\n",
       " 'f9d8a959-71b8-11ee-b347-cc2f714963ed',\n",
       " 'f9d8a95a-71b8-11ee-acd9-cc2f714963ed',\n",
       " 'f9d8a95b-71b8-11ee-afe1-cc2f714963ed',\n",
       " 'f9d8a95c-71b8-11ee-b31b-cc2f714963ed',\n",
       " 'f9d8a95d-71b8-11ee-b35d-cc2f714963ed',\n",
       " 'f9d8a95e-71b8-11ee-aa95-cc2f714963ed',\n",
       " 'f9d8a95f-71b8-11ee-b281-cc2f714963ed',\n",
       " 'f9d8a960-71b8-11ee-8180-cc2f714963ed',\n",
       " 'f9d8a961-71b8-11ee-98ee-cc2f714963ed',\n",
       " 'f9d8a962-71b8-11ee-84f3-cc2f714963ed',\n",
       " 'f9d8a963-71b8-11ee-a852-cc2f714963ed',\n",
       " 'f9d8a964-71b8-11ee-83c0-cc2f714963ed',\n",
       " 'f9d8a965-71b8-11ee-be21-cc2f714963ed',\n",
       " 'f9d8a966-71b8-11ee-9d0b-cc2f714963ed',\n",
       " 'f9d8a967-71b8-11ee-985f-cc2f714963ed',\n",
       " 'f9d8a968-71b8-11ee-946e-cc2f714963ed',\n",
       " 'f9d8a969-71b8-11ee-becd-cc2f714963ed',\n",
       " 'f9d8a96a-71b8-11ee-b05b-cc2f714963ed',\n",
       " 'f9d8a96b-71b8-11ee-bf6c-cc2f714963ed',\n",
       " 'f9d8a96c-71b8-11ee-b15c-cc2f714963ed',\n",
       " 'f9d8a96d-71b8-11ee-b8f3-cc2f714963ed',\n",
       " 'f9d8a96e-71b8-11ee-bf10-cc2f714963ed',\n",
       " 'f9d8a96f-71b8-11ee-94b6-cc2f714963ed',\n",
       " 'f9d8a970-71b8-11ee-aa43-cc2f714963ed',\n",
       " 'f9d8a971-71b8-11ee-abbc-cc2f714963ed',\n",
       " 'f9d8a972-71b8-11ee-9c0c-cc2f714963ed',\n",
       " 'f9d8a973-71b8-11ee-aa10-cc2f714963ed',\n",
       " 'f9d8a974-71b8-11ee-a7a1-cc2f714963ed',\n",
       " 'f9d8a975-71b8-11ee-a11a-cc2f714963ed',\n",
       " 'f9d8a976-71b8-11ee-8f34-cc2f714963ed',\n",
       " 'f9d8a977-71b8-11ee-bed7-cc2f714963ed',\n",
       " 'f9d8a978-71b8-11ee-864d-cc2f714963ed',\n",
       " 'f9d8a979-71b8-11ee-b95b-cc2f714963ed',\n",
       " 'f9d8a97a-71b8-11ee-9265-cc2f714963ed',\n",
       " 'f9d8a97b-71b8-11ee-8ba1-cc2f714963ed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we add all the chunks to the deep lake, along with their metadata\n",
    "db.add_texts(all_texts, all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Chain \n",
    "\n",
    "We then create an instance of RetrievalQAWithSourcesChain using the from_chain_type method. This method takes the following parameters:\n",
    "\n",
    "LLM: This argument expects to receive an instance of a model (GPT-3, in this case) with a temperature of 0. The temperature controls the randomness of the model's outputs - a higher temperature results in more randomness, while a lower temperature makes the outputs more deterministic.\n",
    "chain_type=\"stuff\": This defines the type of chain being used, which influences how the model processes the retrieved documents and generates responses. \n",
    "retriever=db.as_retriever(): This sets up the retriever that will fetch the relevant documents from the Deep Lake database. Here, the Deep Lake database instance db is converted into a retriever using its as_retriever method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Geoffrey Hinton believes that the rapid development of generative AI products is \"racing towards danger\" and that false text, images, and videos created by AI could lead to a situation where average people \"would not be able to know what is true anymore.\" He also expressed concerns about the impact of AI on the job market, as machines could eventually replace roles such as paralegals, personal assistants, and translators.\n",
      "\n",
      "Sources:\n",
      "- https://www.artificialintelligence-news.com/2023/05/02/ai-godfather-warns-dangers-and-quits-google/\n"
     ]
    }
   ],
   "source": [
    "# we create a RetrievalQAWithSourcesChain chain, which is very similar to a\n",
    "# standard retrieval QA chain but it also keeps track of the sources of the\n",
    "# retrieved documents\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm,\n",
    "                                                    chain_type=\"stuff\",\n",
    "                                                    retriever=db.as_retriever())\n",
    "\n",
    "# We generate a response to a query using the chain. The response object is a dictionary containing\n",
    "# an \"answer\" field with the textual answer to the query, and a \"sources\" field containing a string made\n",
    "# of the concatenation of the metadata[\"source\"] strings of the retrieved documents.\n",
    "d_response = chain({\"question\": \"What does Geoffrey Hinton think about recent trends in AI?\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response[\"sources\"].split(\", \"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The chatbot was able to provide an answer to the question, giving a brief overview of Geoffrey Hinton's views on recent trends in AI. The sources provided and the answer traces back to the original articles expressing these views. This process adds a layer of credibility and traceability to the chatbot's responses. The presence of multiple sources also suggests that the chatbot was able to draw information from various documents to provide a comprehensive answer, demonstrating the effectiveness of the RetrievalQAWithSourcesChain in retrieving information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
