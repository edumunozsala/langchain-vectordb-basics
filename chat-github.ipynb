{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with a GitHub Repository\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large language models (LLMs) accomplish a remarkable level of language comprehension during their training process. It enables them to generate human-like text and creates powerful representations from textual data. We already covered leveraging LangChain to use LLMs for writing content with hands-on projects.\n",
    "\n",
    "This lesson will focus on using the language models for generating embeddings from corpora. The mentioned representation will power a chat application that can answer questions from any text by finding the closest data point to an inquiry. This project focuses on finding answers from a GitHub repository’s text files like .md and .txt. So, we will start by capturing data from a GitHub repository and converting it to embeddings. These embeddings will be saved on the Activeloop’s Deep Lake vector database for fast and easy access. The Deep Lake’s retriever object will find the related files based on the user’s query and provide them as the context to the model. Lastly, the model leverages the provided information to the best of its ability to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the lesson is based on the code from the [“Chat with Github Repo”](https://github.com/peterw/Chat-with-Github-Repo/) repository and is organized as follows: \n",
    "\n",
    "1) Processing the Files \n",
    "\n",
    "2) Saving the Embedding \n",
    "\n",
    "3) Retrieving from Database \n",
    "\n",
    "4) Creating an Interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Repository Files\n",
    "\n",
    "In order to access the files in the target repository, the script will clone the desired repository onto your computer, placing the files in a folder named \"repos\". Once we download the files, it is a matter of looping through the directory to create a list of files. It is possible to filter out specific extensions or environmental items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./path/to/cloned/repository\"\n",
    "docs = []\n",
    "file_extensions = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "\t\n",
    "\tfor file in filenames:\n",
    "\t    file_path = os.path.join(dirpath, file)\n",
    "\t\n",
    "\t    if file_extensions and os.path.splitext(file)[1] not in file_extensions:\n",
    "            continue\n",
    "\t\n",
    "    loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "    docs.extend(loader.load_and_split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code above creates a list of all the files in a repository. It is possible to filter each item by extension types like file_extensions=['.md', '.txt'] which only focus on markdown and text files. The original implementation has more filters and a fail-safe approach; Please refer to the complete code.\n",
    "\n",
    "Now that the list of files are created, the split_documents method from the CharacterTextSplitter class in the LangChain library will read the files and split their contents into chunks of 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "splitted_text = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Embeddings\n",
    "\n",
    "Let’s create the database before going through the process of converting texts to embeddings. It is where the integration between LangChain and Deep Lake comes in handy! We initialize the database in cloud using the hub://... format and the OpenAIEmbeddings() from LangChain as the embedding function. The Deep Lake library will iterate through the content and generate the embedding automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_chat_with_gh\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving from Database\n",
    "\n",
    "The last step is to code the process to answer the user’s question based on the database’s information. Once again, the integration of LangChain and Deep Lake simplifies the process significantly, making it exceptionally easy. We need 1) a retriever object from the Deep Lake database using the .as_retriever() method, and 2) a conversational model like ChatGPT using the ChatOpenAI() class.\n",
    "\n",
    "Finally, LangChain’s RetrievalQA class ties everything together! It uses the user’s input as the prompt while including the results from the database as the context. So, the ChatGPT model can find the correct one from the provided context. It is worth noting that the database retriever is configured to gather instances closely related to the user’s query by utilizing cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the DeepLake instance\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Set the search parameters for the retriever\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 100\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "# Create a ChatOpenAI model instance\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a RetrievalQA instance from the model and retriever\n",
    "qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# Return the result of the query\n",
    "qa.run(\"What is the repository's name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Interface\n",
    "\n",
    "Creating a user interface (UI) for the bot to be accessed through a web browser is an optional yet crucial step. This addition will elevate your ideas to new heights, allowing users to engage with the application effortlessly, even without any programming expertise. This repository uses the Streamlit platform, a fast and easy way to build and deploy an application instantly for free. It provides a wide range of widgets to eliminate the need for using backend or frontend frameworks to build a web application.\n",
    "\n",
    "We need a simple UI that accepts the input from the user and shows the conversation in a chat-like interface. Luckily, Streamlit provides both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "# Set the title for the Streamlit app\n",
    "st.title(f\"Chat with GitHub Repository\")\n",
    "\n",
    "# Initialize the session state for placeholder messages.\n",
    "if \"generated\" not in st.session_state:\n",
    "\tst.session_state[\"generated\"] = [\"i am ready to help you ser\"]\n",
    "\n",
    "if \"past\" not in st.session_state:\n",
    "\tst.session_state[\"past\"] = [\"hello\"]\n",
    "\n",
    "# A field input to receive user queries\n",
    "input_text = st.text_input(\"\", key=\"input\")\n",
    "\n",
    "# Search the databse and add the responses to state\n",
    "if user_input:\n",
    "\toutput = qa.run(user_input)\n",
    "\tst.session_state.past.append(user_input)\n",
    "\tst.session_state.generated.append(output)\n",
    "\n",
    "# Create the conversational UI using the previous states\n",
    "if st.session_state[\"generated\"]:\n",
    "\tfor i in range(len(st.session_state[\"generated\"])):\n",
    "\t\tmessage(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "\t\tmessage(st.session_state[\"generated\"][i], key=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code above is straightforward. We call st.text_input() to create text input for users queries. The query will be passed to the previously declared RetrievalQA object, and the results will be shown using the message component. You should store the mentioned code in a Python file (for example, chat.py) and run the following command to see the interface locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!streamlit run ./chat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together\n",
    "\n",
    "As we mentioned previously, the codes in this lesson are available in the “Chat with GitHub Repo,” you can easily fork and run it in 3 simple steps. First, fork the repository and install the required libraries using pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
