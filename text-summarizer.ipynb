{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarizer with LangChain and Deeplake using differents approach to deal with the context\n",
    "\n",
    "During this lesson, we delved into the challenge of summarizing efficiently in the context of the digital age. We will discuss the strategies of \"stuff,\" \"map-reduce,\" and \"refine\" for handling large amounts of text and extracting valuable information.  We also highlighted the customizability of LangChain, allowing personalized prompts, multilingual summaries, and storage of URLs in a Deep Lake vector store. By implementing these advanced tools, you can save time, enhance knowledge retention, and improve your understanding of various topics. Enjoy the tailored experience of data storage and summarization with LangChain\n",
    "\n",
    "\n",
    "## Workflow:\n",
    "\n",
    "1. Transcribe the PDF file to text\n",
    "2. Summarize the transcribed text using LangChain with three different approaches: stuff, refine, and map_reduce.\n",
    "3. Adding multiple URLs to DeepLake database, and retrieving information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with LangChain\n",
    "\n",
    "We first import the necessary classes and utilities from the LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an instance of the RecursiveCharacterTextSplitter\n",
    " class, which is responsible for splitting input text into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Re2G: Retrieve, Rerank, Generate\n",
      "Michael Glass1, Gaetano Rossiello1, Md Faisal Mahbub Chowdhury1,\n",
      "Ankita Rajaram Naik1 2,Pengshan Cai1 2,Alﬁo Gliozzo1\n",
      "1IBM Research AI, Yorktown Heights, NY , USA\n",
      "2University of Massachusetts Amherst, MA, USA\n",
      "Abstract\n",
      "As demonstrated by GPT-3 and T5, transform-\n",
      "ers grow in capability as parameter spaces be-\n",
      "come larger and larger. However, for tasks\n",
      "that require a large amount of knowledge, non-\n",
      "parametric memory allows models to grow dra-\n",
      "matically with a sub-linear increase in compu-\n",
      "tational cost and GPU memory requirements.\n",
      "Recent models such as RAG and REALM\n",
      "have introduced retrieval into conditional gen-\n",
      "eration. These models incorporate neural ini-\n",
      "tial retrieval from a corpus of passages. We\n",
      "build on this line of research, proposing Re2G,\n",
      "which combines both neural initial retrieval\n",
      "and reranking into a BART-based sequence-\n",
      "to-sequence generation. Our reranking ap-\n",
      "proach also permits merging retrieval results\n",
      "from sources with incomparable scores, en-\n",
      "abling an ensemble of BM25 and neural initial\n",
      "retrieval. To train our system end-to-end, we\n",
      "introduce a novel variation of knowledge dis-\n",
      "tillation to train the initial retrieval, reranker\n",
      "and generation using only ground truth on the\n",
      "target sequence output. We ﬁnd large gains in\n",
      "four diverse tasks: zero-shot slot ﬁlling, ques-\n",
      "tion answering, fact checking and dialog, with\n",
      "relative gains of 9% to 34% over the previous\n",
      "state-of-the-art on the KILT leaderboard. We\n",
      "make our code available as open source1.\n",
      "1 Introduction\n",
      "GPT-3 [Brown et al., 2020] and T5 [Raffel et al.,\n",
      "2020] are arguably the most powerful members\n",
      "in a family of deep learning NLP models called\n",
      "transformers. Such models store surprising amount\n",
      "of world knowledge. They have been shown to\n",
      "produce good performance on a range of demand-\n",
      "ing tasks, especially in generating human like texts.\n",
      "However, such large transformers’ capability is\n",
      "tied to the increasingly larger parameter spaces on\n",
      "which they are trained.\n",
      "1https://github.com/IBM/\n",
      "kgi-slot-filling/tree/re2gRecently, there has been work towards trans-\n",
      "formers that make use of non-parametric knowl-\n",
      "edge. REALM (Retrieval Augmented Language\n",
      "Model) [Guu et al., 2020] and RAG (Retrieval Aug-\n",
      "mented Generation) [Lewis et al., 2020b] both use\n",
      "an indexed corpus of passages to support condi-\n",
      "tional generation. By using the corpus as a source\n",
      "of knowledge these models can extend the informa-\n",
      "tion available to the model by tens or even hundreds\n",
      "of gigabytes with a sub-linear scaling in computa-\n",
      "tion cost.\n",
      "These recent advancements, in turn, have\n",
      "been inspired by BART (Bidirectional and Auto-\n",
      "Regressive Transformer) [Lewis et al., 2020a] that\n",
      "combines a Bidirectional Encoder (e.g. BERT [De-\n",
      "vlin et al., 2019]) with an Autoregressive decoder\n",
      "(e.g. GPT [Brown et al., 2020]) into one sequence-\n",
      "to-sequence model.\n",
      "We build on this line of research, pioneered\n",
      "by REALM and RAG, and propose a new ap-\n",
      "proach that we call Re2G(Retrieve, Rerank,\n",
      "Generate), which combines both neural initial re-\n",
      "trieval and reranking into a BART-based sequence-\n",
      "to-sequence generation.\n",
      "There are two particular aspects on which our ap-\n",
      "proach is different from the previous works. Firstly,\n",
      "our reranking approach permits merging retrieval\n",
      "results from sources with incomparable scores, e.g.\n",
      "enabling an ensemble of BM25 and neural initial\n",
      "retrieval. Secondly, to train our system end-to-end,\n",
      "we introduce a novel variation of knowledge dis-\n",
      "tillation to train the initial retrieval, reranker and\n",
      "generation using only ground truth on the target\n",
      "sequence output.\n",
      "The KILT benchmark [Petroni et al., 2021] has\n",
      "been recently introduced to evaluate the capabili-\n",
      "ties of pre-trained language models to address NLP\n",
      "tasks that require access to external knowledge. We\n",
      "evaluate on four diverse tasks from KILT: slot ﬁll-\n",
      "ing, question answering, fact checking and dialog.\n",
      "Figure 1 shows examples of these tasks. Re2GarXiv:2207.06300v1  [cs.CL]  13 Jul 2022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing required modules \n",
    "from pypdf import PdfReader \n",
    "  \n",
    "# creating a pdf reader object \n",
    "reader = PdfReader('data/Retrieve rerank generate.pdf') \n",
    "  \n",
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n",
    "  \n",
    "# Convert \n",
    "# getting a specific page from the pdf file \n",
    "page = reader.pages[0] \n",
    "  \n",
    "# extracting text from page \n",
    "text = page.extract_text() \n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is configured with a chunk_size of 1000 characters, no chunk_overlap, and uses spaces, commas, and newline characters as separators. This ensures that the input text is broken down into manageable pieces, allowing for efficient processing by the language model.\n",
    "\n",
    "We’ll open the text file we’ve saved previously and split the transcripts using .split_text() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re2G: Retrieve, Rerank, Generate\n",
      "Michael Glass1, Gaetano Rossiello1, Md Faisal Mahbub Chowdhury1,\n",
      "Ankita Rajaram Naik1 2,Pengshan Cai1 2,Alﬁo Gliozzo1\n",
      "1IBM Research AI, Yorktown Heights, NY , USA\n",
      "2University of Massachusetts Amherst, MA, USA\n",
      "Abstract\n",
      "As demonstrated by GPT-3 and T5, transform-\n",
      "ers g\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Create a string with the full text\n",
    "text=\"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "    text += '\\n'\n",
    "# Show text\n",
    "print(text[:300])\n",
    "#Split the mopdel\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:4]]\n",
    "# Show count of documentos\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Re2G: Retrieve, Rerank, Generate\\nMichael Glass1, Gaetano Rossiello1, Md Faisal Mahbub Chowdhury1,\\nAnkita Rajaram Naik1 2,Pengshan Cai1 2,Alﬁo Gliozzo1\\n1IBM Research AI, Yorktown Heights, NY , USA\\n2University of Massachusetts Amherst, MA, USA\\nAbstract\\nAs demonstrated by GPT-3 and T5, transform-\\ners grow in capability as parameter spaces be-\\ncome larger and larger. However, for tasks\\nthat require a large amount of knowledge, non-\\nparametric memory allows models to grow dra-\\nmatically with a sub-linear increase in compu-\\ntational cost and GPU memory requirements.\\nRecent models such as RAG and REALM\\nhave introduced retrieval into conditional gen-\\neration. These models incorporate neural ini-\\ntial retrieval from a corpus of passages. We\\nbuild on this line of research, proposing Re2G,\\nwhich combines both neural initial retrieval\\nand reranking into a BART-based sequence-\\nto-sequence generation. Our reranking ap-\\nproach also permits merging retrieval results\\nfrom sources with incomparable', metadata={}),\n",
       " Document(page_content='scores, en-\\nabling an ensemble of BM25 and neural initial\\nretrieval. To train our system end-to-end, we\\nintroduce a novel variation of knowledge dis-\\ntillation to train the initial retrieval, reranker\\nand generation using only ground truth on the\\ntarget sequence output. We ﬁnd large gains in\\nfour diverse tasks: zero-shot slot ﬁlling, ques-\\ntion answering, fact checking and dialog, with\\nrelative gains of 9% to 34% over the previous\\nstate-of-the-art on the KILT leaderboard. We\\nmake our code available as open source1.\\n1 Introduction\\nGPT-3 [Brown et al., 2020] and T5 [Raffel et al.,\\n2020] are arguably the most powerful members\\nin a family of deep learning NLP models called\\ntransformers. Such models store surprising amount\\nof world knowledge. They have been shown to\\nproduce good performance on a range of demand-\\ning tasks, especially in generating human like texts.\\nHowever, such large transformers’ capability is\\ntied to the increasingly larger parameter spaces on\\nwhich they are', metadata={}),\n",
       " Document(page_content='trained.\\n1https://github.com/IBM/\\nkgi-slot-filling/tree/re2gRecently, there has been work towards trans-\\nformers that make use of non-parametric knowl-\\nedge. REALM (Retrieval Augmented Language\\nModel) [Guu et al., 2020] and RAG (Retrieval Aug-\\nmented Generation) [Lewis et al., 2020b] both use\\nan indexed corpus of passages to support condi-\\ntional generation. By using the corpus as a source\\nof knowledge these models can extend the informa-\\ntion available to the model by tens or even hundreds\\nof gigabytes with a sub-linear scaling in computa-\\ntion cost.\\nThese recent advancements, in turn, have\\nbeen inspired by BART (Bidirectional and Auto-\\nRegressive Transformer) [Lewis et al., 2020a] that\\ncombines a Bidirectional Encoder (e.g. BERT [De-\\nvlin et al., 2019]) with an Autoregressive decoder\\n(e.g. GPT [Brown et al., 2020]) into one sequence-\\nto-sequence model.\\nWe build on this line of research, pioneered\\nby REALM and RAG, and propose a new ap-\\nproach that we call Re2G(Retrieve,', metadata={}),\n",
       " Document(page_content='Rerank,\\nGenerate), which combines both neural initial re-\\ntrieval and reranking into a BART-based sequence-\\nto-sequence generation.\\nThere are two particular aspects on which our ap-\\nproach is different from the previous works. Firstly,\\nour reranking approach permits merging retrieval\\nresults from sources with incomparable scores, e.g.\\nenabling an ensemble of BM25 and neural initial\\nretrieval. Secondly, to train our system end-to-end,\\nwe introduce a novel variation of knowledge dis-\\ntillation to train the initial retrieval, reranker and\\ngeneration using only ground truth on the target\\nsequence output.\\nThe KILT benchmark [Petroni et al., 2021] has\\nbeen recently introduced to evaluate the capabili-\\nties of pre-trained language models to address NLP\\ntasks that require access to external knowledge. We\\nevaluate on four diverse tasks from KILT: slot ﬁll-\\ning, question answering, fact checking and dialog.\\nFigure 1 shows examples of these tasks. Re2GarXiv:2207.06300v1  [cs.CL]  13 Jul', metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Document object is initialized with the content of a chunk from the texts list. The [:4] slice notation indicates that only the first four chunks will be used to create the Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paper introduces Re2G, a BART-based sequence-to-sequence generation approach that combines\n",
      "neural initial retrieval and reranking. It allows for a large amount of knowledge to be incorporated\n",
      "into the model with a sub-linear increase in computational cost and GPU memory requirements, and\n",
      "permits merging retrieval results from sources with incomparable data. The system is tested on four\n",
      "diverse tasks and shows large gains of 9-34% over the previous state-of-the-art on the KILT\n",
      "leaderboard. The code is available as open source.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡\n",
    "The textwrap library in Python provides a convenient way to wrap and format plain text by adjusting line breaks in an input paragraph. It is particularly useful when displaying text within a limited width, such as in console outputs, emails, or other formatted text displays. The library includes convenience functions like wrap, fill, and shorten, as well as the TextWrapper class that handles most of the work. If you’re curious, I encourage you to follow this link and find out more, as there are other functions in the textwrap library that can be useful depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following line of code, we can see the prompt template that is used with the map_reduce technique. Now we’re changing the prompt and using another summarization method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print( chain.llm_chain.prompt.template )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff approach\n",
    "\n",
    "The \"stuff\" approach is the simplest and most naive one, in which all the text from the transcribed video is used in a single prompt. This method may raise exceptions if all text is longer than the available context size of the LLM and may not be the most efficient way to handle large amounts of text. \n",
    "We’re going to experiment with the prompt below. This prompt will output the summary as bullet points. Also, we initialized the summarization chain using the stuff as chain_type and the prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Re2G is a new approach that combines neural initial retrieval and reranking into a BART-based sequence-to-sequence generation.\n",
      "- Re2G enables merging retrieval results from sources with incomparable scores, e.g. an ensemble of BM25 and neural initial retrieval.\n",
      "- Re2G is trained end-to-end using a novel variation of knowledge distillation.\n",
      "- Re2G is evaluated on four diverse tasks from KILT: slot filling, question answering, fact checking and dialog.\n",
      "- Re2G achieves large gains in these tasks, with relative gains of 9% to 34% over the previous state-of-the-art on the KILT leaderboard.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template, \n",
    "                        input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, \n",
    "                             chain_type=\"stuff\", \n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary, \n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine approach\n",
    "\n",
    "The 'refine' summarization chain is a method for generating more accurate and context-aware summaries. This chain type is designed to iteratively refine the summary by providing additional context when needed. That means: it generates the summary of the first chunk. Then, for each successive chunk, the work-in-progress summary is integrated with new info from the new chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Re2G is a new model that combines neural initial retrieval and reranking into a BART-based\n",
      "sequence-to-sequence generation. This model allows for a large amount of knowledge to be\n",
      "incorporated into the model with a sub-linear increase in computational cost and GPU memory\n",
      "requirements. Re2G also permits merging retrieval results from sources with incomparable data and\n",
      "scores, enabling an ensemble of BM25 and neural initial retrieval. To train our system end-to-end,\n",
      "we introduce a novel variation of knowledge distillation to train the initial retrieval, reranker\n",
      "and generation using only ground truth on the target sequence output. We find large gains in four\n",
      "diverse tasks from the KILT benchmark: zero-shot slot filling, question answering, fact checking and\n",
      "dialog, with relative gains of 9% to 34% over the previous state-of-the-art. We make our code\n",
      "available as open source.\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeplake as Vector database\n",
    "Now, we’re ready to import Deep Lake and build a database with embedded documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edumu\\anaconda3\\envs\\llm-lc\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.1) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/edumunozsala/langchain_course_youtube_summarizer\n",
      "hub://edumunozsala/langchain_course_youtube_summarizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 1/1 [00:14<00:00\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://edumunozsala/langchain_course_youtube_summarizer', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (4, 1536)  float32   None   \n",
      "    ids      text     (4, 1)      str     None   \n",
      " metadata    json     (4, 1)      str     None   \n",
      "   text      text     (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2cc11c53-6c3e-11ee-900d-cc2f714963ed',\n",
       " '2cc11c54-6c3e-11ee-a955-cc2f714963ed',\n",
       " '2cc11c55-6c3e-11ee-b861-cc2f714963ed',\n",
       " '2cc11c56-6c3e-11ee-808d-cc2f714963ed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "# create Deep Lake dataset\n",
    "my_activeloop_org_id = \"edumunozsala\"\n",
    "my_activeloop_dataset_name = \"langchain_course_youtube_summarizer\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve the information from the database, we’d have to construct a retriever object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance metric determines how the Retriever measures \"distance\" or similarity between different data points in the database. By setting distance_metric to 'cos', the Retriever will use cosine similarity as its distance metric. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. It's often used in information retrieval to measure the similarity between documents or pieces of text. Also, by setting 'k' to 4, the Retriever will return the 4 most similar or closest results according to the distance metric when a search is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Summarized answer in bullter points:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can use the chain_type_kwargs argument to define the custom prompt and for chain type the ‘stuff’  variation was picked. You can perform and test other types as well, as seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Re2G is a new approach that combines both neural initial retrieval and reranking into a BART-based sequence-to-sequence generation.\n",
      "- Re2G permits merging retrieval results from sources with incomparable scores, enabling an ensemble of BM25 and neural initial retrieval.\n",
      "- Re2G is trained using a novel variation of knowledge distillation to train the initial retrieval, reranker and generation using only ground truth on the target sequence output.\n",
      "- Re2G has been evaluated on four diverse tasks from KILT: slot filling, question answering, fact checking and dialog, with relative gains of 9% to 34% over the previous state-of-the-art on the KILT leaderboard.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "print( qa.run(\"Summarize the mentions of Re2G \") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can always tweak the prompt to get the desired result, experiment more with modified prompts using different types of chains and find the most suitable combination. Ultimately, the choice of strategy depends on the specific needs and constraints of your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
